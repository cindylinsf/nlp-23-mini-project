{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Mini Project // Term 1 // Cindy Lin\n",
    "\n",
    "### Part 1: Web Scraping\n",
    "- I used the code from the class notebook but modified the code to scrape Cryobank's website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_donor_info(donor_number):\n",
    "    # The Cryobank donor urls and pages follow a consistent structure\n",
    "    donor_url = f'https://www.cryobank.com/donor/{donor_number}'\n",
    "\n",
    "    # Make a request to the Cryobank server and check to see we get a response\n",
    "    response = requests.get(donor_url)\n",
    "    if response.status_code != 200:\n",
    "        return \"Failed to retrieve the page.\"\n",
    "\n",
    "    # Use beatiful soup to parse the HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Using the Developer Tool on my web browser, I found the content I want in the 3rd <h2> tag\n",
    "    # In this part of the code, the function will find all the <h2> tag on the page\n",
    "    h2_info = \"\"\n",
    "    h2_tags = soup.find_all('h2')\n",
    "\n",
    "    # The if/else condition will pull only the 3rd <h2> tag by specifying\n",
    "    # if the <h2> tag index numbers are larger or equal to 3, pull only the second index <h2> tag\n",
    "    if len(h2_tags) >= 3:\n",
    "        h2_info = h2_tags[2].get_text()\n",
    "    else:\n",
    "        h2_info = \"No third <h2> tag found\"\n",
    "\n",
    "    # Using the Developer Tool in my browser, I found the content I want in the 13th <p> tag\n",
    "    p_info = \"\"\n",
    "    p_tags = soup.find_all('p')\n",
    "\n",
    "    # Using the same process for <h2>, we are scraping the 13th <p>\n",
    "    if len(p_tags) >= 13:\n",
    "        p_info = p_tags[12].get_text()\n",
    "    else:\n",
    "        p_info = \"No 13th <p> tag found\"\n",
    "    \n",
    "    return h2_info, p_info\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Asian Donors:\n",
    "- 127 donors available. \n",
    "- It looks like the donor numbers are randomly assigned, so I manually entered all the donor numbers from the search result.\n",
    "- When donor numbers started with a zero, I added '' around the numbers. Otherwise, the program returns an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "donor_numbers = [18230, 18579, 18784, 17858, 18256, 17801, 18435, 18027, 17711, 18100, 16948, 17951, 17566, 17651, 16973, 16534, 16482, 17962, 17019, 17530, 17866, 17589, 17608, 17564, 17409, 17055, 17067, 17121, 17096, 16805, 16928, 16816, 16924, 16910, 16814, 16643, 16691, 16622, 16768, 16417, 16845, 16630, 16371, 16456, 16451, 16344, 16265, 16247, 16354, 16187, 15928, 15961, 15855, 15867, 15862, 15830, 15784, 15857, 15789, 15782, 15721, 15647, 15557, 15512, 15516, 15472, 15253, 15262, 15063, 15096, 14935, 14724, 14747, 14759, 14765, 14593, 14455, 14578, 14487, 14548, 14185, 14232, 14143, 14109, 14045, 13987, 13909, 13704, 13910, 13820, 13608, 13229, 13240, 13210, 13009, 12337, 12180, 11576, 11560, 11432, 11361, 11346, 11312, 11278, 11245, 11060, 11097, 11048, 11011, '05686', '05626', '02435', '03689', '03637', '05502', '05446', '05272', '00756', '02110', '00644', '03046', '03056', '00890', '00763', '00894', '01136', '00701']\n",
    "for donor_number in donor_numbers:\n",
    "    h2_info, p_info = extract_donor_info(donor_number)\n",
    "    # print(\"Donor Number \" + str(donor_number))\n",
    "    print(h2_info)\n",
    "    print(p_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Asian Donors info scraped to .txt file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../nlp-23-mini-project/my-data/Asian-Donors.txt\", \"a\") as myfile:\n",
    "    for donor_number in donor_numbers:\n",
    "        h2_info, p_info = extract_donor_info(donor_number)\n",
    "        myfile.write(h2_info + \"\\n\")\n",
    "        myfile.write(p_info + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Black or African American Donors:\n",
    "\n",
    "- Only 8 donor profiles are available. \n",
    "- The same process is followed to create dataset for this demographic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rising Opera Star\n",
      "Born with a singing voice that would be the envy of Pavarotti and Bocelli, Donor 18507’s talent is as impressive as his dimples are big, his smile bright, and his almond-shaped eyes beautiful. Having discovered his passion for music as a 14-year-old in his church choir, this aspiring opera singer is now working on a performing arts degree (3.7 GPA). Ambitious, kind-hearted, and supremely self-confident, his many creative outlets range from acting and dancing to cooking and repairing motorcycles.\n",
      "Caribbean Charmer\n",
      "With dark, long lashes, and dimples, Donor 18552 gets his broad shoulders from competitive weightlifting when he was younger. He graduated from high school two years early (with an associate degree), then got a bachelor’s in psychology and studied post grad — with a 4.0. He loves the arts (he’s pursuing acting) but is also very scientific. His favorite holiday is Thanksgiving. In his family it’s a big party with a “turkey-off” using the recipes of his Trinidadian and Jamaican sides of the family.\n",
      "MVP Grad Student\n",
      "Standing 6’2”, this MVP athlete is the kind of cool, confident individual who thrives on high-pressure situations. He’s always ready to take the final shot during close basketball games, lead group presentations, and speak up at events where audience participation is requested. A talented artist and storyteller working on his master's in media studies, Donor 17841 is excitedly exploring new horizons in digital photography, video, animation, music production, and 3D design.\n",
      "Music Moves Him\n",
      "Adult Photos Available\n",
      "All-Star Video Game Designer\n",
      "Donor 17576 is a high school football hero (defensive MVP and all-star conference player) turned video game designer in college. Calm and decisive, he bases his decisions on their potential impact. He cares about his community and inclusivity, being most proud of his positive influence on the kids he tutors. He believes art should encourage thought. And nothing makes him laugh quite like a dog being goofy or a toddler trying to explain their side of the story.\n",
      "6'4'' of Amazing\n",
      "He’s a brilliant mathematician who considers Euler’s formula “elegant and beautiful” … two words that could easily be used to describe his own amazing singing voice and wonderful mind. Standing 6’4” with an athletic build, this talented singer was nicknamed “Boom Boom” as a child for his high-energy, fun-loving ways. A much calmer adult, Donor 17064 is a friendly physics grad (3.7 GPA) who enjoys working out, hanging with friends, visiting museums, playing basketball, and writing fiction.\n",
      "A Family Man\n",
      "Donor 16733 is sincere, outgoing, and warm-hearted. When he’s not playing scrabble with this beloved family, you can catch him volunteering in his local community. He’s bilingual with a passion for traveling.\n",
      "Handsome Haitian American\n",
      "Donor 5346 stands 6’2” tall, weighs 282 lbs, has kind brown eyes, dark skin, and curly black hair. He was born in America to Haitian parents. He’s open-minded, a good listener, focuses intently on tasks, and tends to be a leader rather than a follower. An undergraduate majoring in psychology, he’s a fast learner, gets better than average grades, and plans to attend medical school to become a physician. His dream destinations include Europe and the Caribbean, for the culture and the “rampant beauty.”\n"
     ]
    }
   ],
   "source": [
    "donor_numbers = [18507,18552, 17841, 16323, 17576, 17064, 16733, '05346']\n",
    "for donor_number in donor_numbers:\n",
    "    h2_info, p_info = extract_donor_info(donor_number)\n",
    "    print(h2_info)\n",
    "    print(p_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Black or African American Donors info scraped to .txt file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print to txt\n",
    "\n",
    "with open(\"../nlp-23-mini-project/my-data/Black-Donors.txt\", \"a\") as myfile:\n",
    "    for donor_number in donor_numbers:\n",
    "        h2_info, p_info = extract_donor_info(donor_number)\n",
    "        myfile.write(h2_info + \"\\n\")\n",
    "        myfile.write(p_info + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Hispanic or Latino Donors:\n",
    "\n",
    "- Only 25 donor profiles are available. \n",
    "- The same process is followed to create dataset for this demographic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staff Favorite\n",
      "\n",
      "Academic Marathoner\n",
      "After slaying high school with a 4.2 GPA and honors, Donor 18689 is now studying human biology and society in college. He describes himself as an empathetic and sensitive, but he also has a competitive side and loves debating movies or playing board games with friends — and winning! He stays fit by running and even completed his first marathon, recently. He’s a pasta and sandwiches kind of guy with jet black hair and a cute chin dimple. One day, he hopes to travel to Seattle to see the Space Needle.\n",
      "ABOUT THIS DONOR\n",
      "Our 90-day Donor Information Subscriptions provide access to all available information on every donor for one low fee. Level 2 and Level 3 items are view only and NOT DOWNLOADABLE. Subscriptions are non-refundable.\n",
      "Poli-Sci Marathoner\n",
      "Donor 17858 is the one all of his friends go to for advice. He has a calm, caring personality, a warm smile, and thick shiny hair straight out of a shampoo commercial. A former varsity wrestler, he now runs marathons to stay fit. After falling in love with politics at a young age, he went on to get a poli-sci B.A., and now works as a political consultant. A lifelong foodie, he expresses himself by cooking for friends. Check out his nori chicken recipe in the Express Yourself section of his profile!\n",
      "Dancing Donor\n",
      "One look at the beautiful, smiling, brown-eyed boy in his childhood photos may be all you need to realize that he's 'the one.' Born in Ecuador but now residing in the United States, Donor 18383 is a graduate of an overseas medical school. He is currently working as a postgraduate research fellow while preparing for the US Medical Licensing Exam. A gifted athlete, he especially enjoys basketball, tennis, football, and swimming. Dancing, cooking, and hiking are his favorite ways to relax.\n",
      "Caribbean Charmer\n",
      "With dark, long lashes, and dimples, Donor 18552 gets his broad shoulders from competitive weightlifting when he was younger. He graduated from high school two years early (with an associate degree), then got a bachelor’s in psychology and studied post grad — with a 4.0. He loves the arts (he’s pursuing acting) but is also very scientific. His favorite holiday is Thanksgiving. In his family it’s a big party with a “turkey-off” using the recipes of his Trinidadian and Jamaican sides of the family.\n",
      "Master Storyteller\n",
      "With his chocolate-brown eyes and coffee-brown hair, Donor 18107 is a 6’2” charmer whose masterful Dungeons and Dragons storytelling skills rival those of J.R.R. Tolkien for inventiveness and excitement. He sees his B.S. in Political Science as the first step toward a future career in politics. Friends say he's 'a compassionate and selfless individual who puts the needs of many before his own.' Loyal and carefree, Donor 18107 loves everything about basketball — playing, refereeing games, coaching, everything!\n",
      "Man of So Many Talents\n",
      "We really like this artist’s vibe. Outgoing and always ready for adventure, Donor 17205 is a good-looking, bilingual (English/Spanish) psych major with an amazing head of hair (it’s must-see). Self-described as a “Type B” personality, nothing stresses out this talented musician/singer and wildly creative painter who’s influenced by Pollock but paints like Basquiat.\n",
      "Dream and Achiever\n",
      "Growing up in El Salvador in the loving care of two his grandmothers, Donor 16828 joined his parents in the U.S. at 15. Quickly settling into his new home, he became an MVP high school soccer player and went on to win multiple academic scholarships, allowing him to become the first in his family to attend college. Intelligent and very ambitious, he believes in making the most of every opportunity, whether it’s working on his B.S. in International Relations or helping others succeed.\n",
      "Compassionate Life Coach\n",
      "\n",
      "To Business and Beyond!\n",
      "A Latino business/computer systems major, Donor 17297’s feet are on the ground and his head’s in the stars — he hopes to see the aurora borealis. An extrovert who prefers 80’s new wave, he advises “Don’t let others judge you, express yourself.” And “Toy Story” returns him to his childhood — playing basketball, hanging with friends, and tennis with his dad.\n",
      "Mad Technical Skills\n",
      "Donor 16889 is a fun-loving, drum-playing computer whiz who’s the go-to-guy that keeps critical IT networks up and running in times of trouble. He's charismatic, personable, and has a great smile. (Adult Photos Available) Passionate about helping others, he works with a non-profit that provides food to those in need. He also shows deep compassion toward his family and friends. Outgoing by nature, he enjoys snowboarding, volunteering, building computers (naturally), and drone photography.\n",
      "Animal Lover, People Person\n",
      "If you were ever lost in the jungle, Donor 16928 is the man who would get you back home safe, sound, and laughing about your amazing adventure. Combining the skills of Angus MacGyver with the charm of Steve Erwin this outgoing future veterinarian is as handy as he is smart and cool-headed. With the kind of encyclopedic knowledge of wildlife that could win millions on Jeopardy, this talented documentary filmmaker’s many interests range from searching for rare sea life while snorkeling in Hawaii to his never-ending quest to find the world’s best bao.\n",
      "Peace, Love, and Music Man\n",
      "Donor 16783 says he’s always planning something. With a BFA in musical theater, he has perfect pitch, loves to laugh, and always gets a laugh. People are surprised this guy has been dancing since he was seven (including ballet). A former wrestler, he still enjoys working out and prefers hot yoga for stress. His also loves “The Sandlot’s” Benny the Jet and breakfast cereal. While never afraid to stand up for himself, he believes in peace and love above all else.\n",
      "Competitive Scientist\n",
      "\n",
      "Jack of All Trades\n",
      "No matter the task – whether it’s growing beautiful hybrid flowers in his amazing garden or cooking an unforgettable cherry braised lamb dinner for friends – this jack of all trades is up to the challenge. A passionate gardener, Donor 16478 is a true Renaissance man whose many talents include building furniture, playing guitar, creating irrigation systems, repairing cars – the list goes on. An autodidact who loves the outdoors, this military veteran enjoys hiking and time spent at the beach.\n",
      "Articulate Magician\n",
      "Tall and handsome with dark wavy hair, this political science major started doing magic as a way to take a break from his rigorous classes. Articulate, well-educated, and a lover of words, he was involved in theatre from a young age and enjoys creating live experiences for others. When he’s not busy writing magic show scripts and short fiction, he enjoys cooking, ultimate Frisbee, and playing board games. His unique talents include fencing (Junior Olympics qualifier), solving political questions, and sailing.\n",
      "On the Right Track\n",
      "Tall with dimples and a warm heart, this sociology student enjoys tutoring kids and being a mentor to his little brother. Donor 16158 has a strong work ethic. He credits his mom with inspiring him to be a hard worker and teaching him new skills like cooking and making floral arrangements. Very focused and motivated, he achieved degrees in Behavioral Science, Humanities, and Math, with a goal of becoming a college counselor. He is also very determined in his athletic pursuits. This track and field star won the title of high school runner of the year. He still enjoys running, working out, and lifting weights. Family is very important to him and he hopes to provide more for them in the future.\n",
      "Loves to Help\n",
      "\n",
      "Natural Protector\n",
      "This donor views himself as an optimistic positive person who will eagerly take the initiative in a small or large group to break the ice – just like his dad. He values his family, and cherishes the conversations he has with his mother, who has always been his number one fan. Education is a staple in his life; he doesn’t just feel like he is solely accomplishing something for himself, but has his entire family’s legacy in mind. Donor 15079 pays close attention to small details; he can recall protecting his mother’s garden during a birthday party as a child, where all of his friends were running around being crazy. While he is not an avid risk taker, he is able to put his fears aside to accomplish his goals. Even if that means facing his fears head on, by showing a little girl his bravery and riding a roller coaster.\n",
      "DJ Dad\n",
      "14484 grew up in Mexico, went to boarding school in Germany, and worked in Canada! This international jet-setter has finally settled down in the U.S. with his young son and a great job in graphic design. He’s an awesome dad, and loves nothing more than to joke around, play outside, and listen to music with his child—he’s got great taste in tunes, and works on producing and DJing his own songs in his free time. In addition, this thrill-seeker loves motocross, wakeboarding, surfing, and skiing!\n",
      "Hands-On\n",
      "14377 loves working with his hands. After he got his first truck as a teenager, he developed a love for tinkering with cars, and goes to the garage whenever he needs to relax. After graduating with biology and chemistry majors, he entered the work force as a surgical assistant, helping his father at his dental practice. He gets a lot of hands-on experience with patients, who love his laid-back nature and dazzling smile. This handsome, 6’3’’ athlete loves to stay active whenever he can, and particularly enjoys snow sports like skiing and snowboarding, basketball, and football.\n",
      "The Complete Package\n",
      "Donor 12785 has rappelled down a canyon, sailed to an island on his own, bungee jumped off a bridge, and climbed a 1,000 foot rock formation – and that’s all just one of his summers. This laid-back yet enthusiastic risk-taker has a muscular build from his weight training, and also enjoys swimming and running. A math whiz civil engineer, he can fix just about anything under the sun, but don’t think he’s just an athletic, adventurous, intelligent guy; he’s also artistic, enjoying both drawing and graphic design.\n",
      "Latin Lover\n",
      "Donor 11206’s Colombian parents instilled in him a strong work ethic, a love of family and a zest for life. He’s 5’9”/178 lbs, with brown eyes, thick black hair, and boyish good looks. Weightlifting, soccer, baseball, football, and basketball keep him active and healthy, and he’s a skilled artist with a special talent for working with 3-D models. He’s majoring in Political Science and plans to own his own company and travel throughout Europe.\n",
      "Big Heart, Big Plans\n",
      "Fascinated by science, technology, and medicine, Donor 16359 is pursuing a second degree in nursing. His compassion and motivation for helping others extend beyond just caring for humans. This aquarist is an avid fish keeper and loves taking care of an ecosystem. Skilled in a variety of fields, he enjoys doing research, cycling, painting, and cooking. His favorite dessert to bake is pineapple upside-down cake. Although he says he’s a bit shy, he thrives when it comes to public speaking and is passionate about learning new things.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "donor_numbers = [18230, 18689, 18510, 17858, 18383, 18552, 18107, 17205, 16828, 16432, 17297, 16889, 16928, 16783, 16138, 16478, 16388, 16158, 16104, 15079, 14484, 14377, 12785, 11206, 16359]\n",
    "for donor_number in donor_numbers:\n",
    "    h2_info, p_info = extract_donor_info(donor_number)\n",
    "    print(h2_info)\n",
    "    print(p_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Hispanic or Latino Donors info scraped to .txt file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print to txt\n",
    "\n",
    "with open(\"../nlp-23-mini-project/my-data/Hipanic-Donors.txt\", \"a\") as myfile:\n",
    "    for donor_number in donor_numbers:\n",
    "        h2_info, p_info = extract_donor_info(donor_number)\n",
    "        myfile.write(h2_info + \"\\n\")\n",
    "        myfile.write(p_info + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Caucasian Donors:\n",
    "\n",
    "- 107 donor profiles are available. \n",
    "- The same process is followed to create dataset for this demographic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "donor_numbers = [18230, 18579, 17958, 18784, 17858, 18436, 17965, 18306, 17917, 18599, 18552, 18107, 17981, 17841, 18163, 16495, 16403, 16701, 16620, 16948, 16386, 18127, 16828, 17651, 16908, 17894, 17052, 18088, 16677, 17576, 17954, 17800, 17859, 17593, 17690, 17728, 17416, 17457, 17564, 17212, 17277, 16889, 17279, 17187, 16903, 16847, 16928, 17089, 16783, 16555, 16824, 16477, 16630, 16138, 16478, 16317, 16337, 16388, 16027, 16049, 16088, 16141, 15882, 15896, 15935, 15808, 15713, 15472, 15402, 15412, 15230, 14455, 14484, 14420, 14259, 14176, 13595, 13608, 13317, 13009, 13025, 12873, 12785, 12824, 12594, 12685, 12632, 11740, 11422, 11283, 11294, 11291, 11061, 11038, 11030, '05766', '05694', '05679', '05642', '05497', '03559', '05441', '03461', '05269', '05268', '03291', '00790'\n",
    "]\n",
    "for donor_number in donor_numbers:\n",
    "    h2_info, p_info = extract_donor_info(donor_number)\n",
    "    print(h2_info)\n",
    "    print(p_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Caucasian Donor info scraped to .txt file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print to txt\n",
    "\n",
    "with open(\"../nlp-23-mini-project/my-data/Caucasian-Donors.txt\", \"a\") as myfile:\n",
    "    for donor_number in donor_numbers:\n",
    "        h2_info, p_info = extract_donor_info(donor_number)\n",
    "        # myfile.write(f\"Donor Number {donor_number}\\n\")\n",
    "        myfile.write(h2_info + \"\\n\")\n",
    "        myfile.write(p_info + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cleaning up the scraped text & generate unique keywords used. \n",
    "\n",
    "1. Manually removing irrelevant text: \n",
    "- The company uses AI generated stock photos to make the donor profiles more enticing, so each descriptions has a discalimer: \"The pictures provided are meant to represent the interests and/or activities of the donor. These are not photographs of the donor engaging in the activities.\"\n",
    "- On older profiles, there were photo disclaimers like \"Our 90-day Donor Information Subscriptions provide access to all available information on every donor for one low fee. Level 2 and Level 3 items are view only and NOT DOWNLOADABLE. Subscriptions are non-refundable.\" These are removed as well.\n",
    "- With each filtered search, all of them has a \"Staff Favorite\" profile pinned on the top. The profile didn't have any descriptions in them, so those are removed as well. \n",
    "\n",
    "2. Lowercasing all the text\n",
    "\n",
    "3. Tokenization\n",
    "\n",
    "4. Removing special characters and punctuation\n",
    "\n",
    "5. Removing Stop Words: In additions to removing common words like \"the,\" \"is,\" \"in\", etc., I will also remove the word \"donor\" since that's going to be appearing at a very frequent level in each of the dataset. I also ended up removing more words like \"he\", \"his\", etc. that doesn't contribute to the results. Since I want to see the most frequently used keywords in the descriptions.\n",
    "\n",
    "6. Lemmatization\n",
    "\n",
    "7. Numerical Data: A lot of the donor descriptions include the donor numbers. So they will be excluded during this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, I used the code from class. \n",
    "- I also used ChatGPT help me with modifying the code and troubleshoot errors from consolidating the code. \n",
    "- I was having an issue of getting \"he's\" in the results and Aarav suggested installing clean-text for a more powerful filtering. Which worked great and also removed .lower() function in the original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install clean-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized content for Asian Donors:\n",
      "Unique words: (2132,)\n",
      "[('enjoys', 45), ('loves', 36), ('time', 35), ('school', 33), ('friends', 31), ('playing', 30), ('basketball', 30), ('eyes', 27), ('world', 27), ('life', 26), ('family', 25), ('high', 23), ('hair', 22), ('brown', 22), ('college', 21), ('smile', 21), ('career', 21), ('great', 21), ('math', 21), ('tall', 20), ('work', 19), ('love', 18), ('new', 18), ('like', 18), ('working', 18), ('earned', 17), ('good', 17), ('black', 16), ('personality', 16), ('major', 16), ('outgoing', 16), ('engineering', 16), ('chinese', 16), ('english', 16), ('proud', 16), ('student', 15), ('business', 15), ('friendly', 15), ('future', 15), ('tennis', 15), ('swimming', 15), ('creative', 15), ('people', 15), ('gpa', 14), ('computer', 14), ('china', 14), ('piano', 14), ('goal', 14), ('fit', 13), ('plays', 13)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cleantext import clean\n",
    "\n",
    "# File path\n",
    "file_path = \"../nlp-23-mini-project/my-data/Asian-Donors.txt\"\n",
    "\n",
    "# Read the file content\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Clean the text using clean-text\n",
    "cleaned_text = clean(content, \n",
    "                     fix_unicode=True, \n",
    "                     to_ascii=True, \n",
    "                     lower=True, \n",
    "                     no_line_breaks=True, \n",
    "                     no_urls=True, \n",
    "                     no_emails=True, \n",
    "                     no_phone_numbers=True, \n",
    "                     no_numbers=True, \n",
    "                     no_digits=True, \n",
    "                     no_currency_symbols=True, \n",
    "                     no_punct=True, \n",
    "                     replace_with_url=\"\", \n",
    "                     replace_with_email=\"\", \n",
    "                     replace_with_phone_number=\"\", \n",
    "                     replace_with_number=\"\", \n",
    "                     replace_with_digit=\"\")\n",
    "\n",
    "# Tokenize the cleaned text using NLTK word_tokenize\n",
    "tokens = word_tokenize(cleaned_text)\n",
    "\n",
    "# Define the words to remove\n",
    "words_to_remove = [\"he\", \"his\", \"donor\", \"man\", \"hes\"]\n",
    "\n",
    "# Remove the words and any empty strings\n",
    "tokens_without_removed_words = [word for word in tokens if word not in words_to_remove and word]\n",
    "\n",
    "# Remove stop words\n",
    "tokens_cleaned = [t for t in tokens_without_removed_words if t not in _stop_words.ENGLISH_STOP_WORDS]\n",
    "\n",
    "# Get unique words and their count\n",
    "vocab = np.unique(tokens_cleaned)\n",
    "print(\"Tokenized content for Asian Donors:\")\n",
    "print(\"Unique words:\", vocab.shape)\n",
    "print(Counter(tokens_cleaned).most_common(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized content for Black Donors:\n",
      "Unique words: (245,)\n",
      "[('family', 4), ('music', 3), ('working', 3), ('school', 3), ('grad', 3), ('mvp', 3), ('video', 3), ('opera', 2), ('born', 2), ('singing', 2), ('voice', 2), ('dimples', 2), ('big', 2), ('eyes', 2), ('beautiful', 2), ('passion', 2), ('singer', 2), ('arts', 2), ('degree', 2), ('gpa', 2), ('acting', 2), ('caribbean', 2), ('dark', 2), ('gets', 2), ('high', 2), ('psychology', 2), ('standing', 2), ('kind', 2), ('basketball', 2), ('talented', 2), ('allstar', 2), ('game', 2), ('designer', 2), ('community', 2), ('amazing', 2), ('boom', 2), ('playing', 2), ('haitian', 2), ('rising', 1), ('star', 1), ('envy', 1), ('pavarotti', 1), ('bocelli', 1), ('s', 1), ('talent', 1), ('impressive', 1), ('smile', 1), ('bright', 1), ('almondshaped', 1), ('having', 1)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cleantext import clean\n",
    "\n",
    "# File path\n",
    "file_path = \"../nlp-23-mini-project/my-data/Black-Donors.txt\"\n",
    "\n",
    "# Read the file content\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Clean the text using clean-text\n",
    "cleaned_text = clean(content, \n",
    "                     fix_unicode=True, \n",
    "                     to_ascii=True, \n",
    "                     lower=True, \n",
    "                     no_line_breaks=True, \n",
    "                     no_urls=True, \n",
    "                     no_emails=True, \n",
    "                     no_phone_numbers=True, \n",
    "                     no_numbers=True, \n",
    "                     no_digits=True, \n",
    "                     no_currency_symbols=True, \n",
    "                     no_punct=True, \n",
    "                     replace_with_url=\"\", \n",
    "                     replace_with_email=\"\", \n",
    "                     replace_with_phone_number=\"\", \n",
    "                     replace_with_number=\"\", \n",
    "                     replace_with_digit=\"\")\n",
    "\n",
    "# Tokenize the cleaned text using NLTK word_tokenize\n",
    "tokens = word_tokenize(cleaned_text)\n",
    "\n",
    "# Define the words to remove\n",
    "words_to_remove = [\"he\", \"his\", \"donor\", \"man\", \"hes\"]\n",
    "\n",
    "# Remove the words and any empty strings\n",
    "tokens_without_removed_words = [word for word in tokens if word not in words_to_remove and word]\n",
    "\n",
    "# Remove stop words\n",
    "tokens_cleaned = [t for t in tokens_without_removed_words if t not in _stop_words.ENGLISH_STOP_WORDS]\n",
    "\n",
    "# Get unique words and their count\n",
    "vocab = np.unique(tokens_cleaned)\n",
    "print(\"Tokenized content for Black Donors:\")\n",
    "print(\"Unique words:\", vocab.shape)\n",
    "print(Counter(tokens_cleaned).most_common(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized content for Hispanic or Latino Donors:\n",
      "Unique words: (671,)\n",
      "[('loves', 12), ('enjoys', 11), ('friends', 8), ('family', 7), ('school', 6), ('hair', 6), ('love', 6), ('cooking', 6), ('working', 6), ('like', 6), ('just', 6), ('playing', 5), ('running', 5), ('political', 5), ('basketball', 5), ('science', 5), ('high', 4), ('skills', 4), ('new', 4), ('helping', 4), ('life', 4), ('dad', 4), ('college', 3), ('competitive', 3), ('games', 3), ('guy', 3), ('hopes', 3), ('smile', 3), ('young', 3), ('went', 3), ('works', 3), ('dancing', 3), ('football', 3), ('favorite', 3), ('gets', 3), ('got', 3), ('big', 3), ('future', 3), ('talents', 3), ('outgoing', 3), ('major', 3), ('amazing', 3), ('s', 3), ('great', 3), ('passionate', 3), ('lover', 3), ('work', 3), ('academic', 2), ('marathoner', 2), ('biology', 2)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cleantext import clean\n",
    "\n",
    "# File path\n",
    "file_path = \"../nlp-23-mini-project/my-data/Hipanic-Donors.txt\"\n",
    "\n",
    "# Read the file content\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Clean the text using clean-text\n",
    "cleaned_text = clean(content, \n",
    "                     fix_unicode=True, \n",
    "                     to_ascii=True, \n",
    "                     lower=True, \n",
    "                     no_line_breaks=True, \n",
    "                     no_urls=True, \n",
    "                     no_emails=True, \n",
    "                     no_phone_numbers=True, \n",
    "                     no_numbers=True, \n",
    "                     no_digits=True, \n",
    "                     no_currency_symbols=True, \n",
    "                     no_punct=True, \n",
    "                     replace_with_url=\"\", \n",
    "                     replace_with_email=\"\", \n",
    "                     replace_with_phone_number=\"\", \n",
    "                     replace_with_number=\"\", \n",
    "                     replace_with_digit=\"\")\n",
    "\n",
    "# Tokenize the cleaned text using NLTK word_tokenize\n",
    "tokens = word_tokenize(cleaned_text)\n",
    "\n",
    "# Define the words to remove\n",
    "words_to_remove = [\"he\", \"his\", \"donor\", \"man\", \"hes\"]\n",
    "\n",
    "# Remove the words and any empty strings\n",
    "tokens_without_removed_words = [word for word in tokens if word not in words_to_remove and word]\n",
    "\n",
    "# Remove stop words\n",
    "tokens_cleaned = [t for t in tokens_without_removed_words if t not in _stop_words.ENGLISH_STOP_WORDS]\n",
    "\n",
    "# Get unique words and their count\n",
    "vocab = np.unique(tokens_cleaned)\n",
    "print(\"Tokenized content for Hispanic or Latino Donors:\")\n",
    "print(\"Unique words:\", vocab.shape)\n",
    "print(Counter(tokens_cleaned).most_common(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized content for Caucasian Donors:\n",
      "Unique words: (1868,)\n",
      "[('loves', 36), ('enjoys', 34), ('eyes', 24), ('school', 24), ('playing', 24), ('music', 23), ('guy', 23), ('creative', 22), ('high', 20), ('family', 20), ('new', 20), ('smile', 19), ('great', 19), ('hair', 18), ('people', 18), ('working', 18), ('time', 17), ('friends', 17), ('college', 15), ('love', 14), ('sense', 14), ('good', 13), ('writing', 13), ('athlete', 13), ('life', 13), ('computer', 13), ('rock', 12), ('athletic', 12), ('brown', 12), ('favorite', 12), ('gpa', 11), ('passionate', 11), ('believes', 11), ('just', 11), ('masters', 11), ('engineering', 11), ('sports', 11), ('handsome', 11), ('dark', 11), ('career', 11), ('like', 11), ('humor', 11), ('major', 11), ('beautiful', 10), ('plays', 10), ('dimples', 10), ('close', 10), ('degree', 10), ('help', 10), ('include', 10)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cleantext import clean\n",
    "\n",
    "# File path\n",
    "file_path = \"../nlp-23-mini-project/my-data/Caucasian-Donors.txt\"\n",
    "\n",
    "# Read the file content\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Clean the text using clean-text\n",
    "cleaned_text = clean(content, \n",
    "                     fix_unicode=True, \n",
    "                     to_ascii=True, \n",
    "                     lower=True, \n",
    "                     no_line_breaks=True, \n",
    "                     no_urls=True, \n",
    "                     no_emails=True, \n",
    "                     no_phone_numbers=True, \n",
    "                     no_numbers=True, \n",
    "                     no_digits=True, \n",
    "                     no_currency_symbols=True, \n",
    "                     no_punct=True, \n",
    "                     replace_with_url=\"\", \n",
    "                     replace_with_email=\"\", \n",
    "                     replace_with_phone_number=\"\", \n",
    "                     replace_with_number=\"\", \n",
    "                     replace_with_digit=\"\")\n",
    "\n",
    "# Tokenize the cleaned text using NLTK word_tokenize\n",
    "tokens = word_tokenize(cleaned_text)\n",
    "\n",
    "# Define the words to remove\n",
    "words_to_remove = [\"he\", \"his\", \"donor\", \"man\", \"hes\"]\n",
    "\n",
    "# Remove the words and any empty strings\n",
    "tokens_without_removed_words = [word for word in tokens if word not in words_to_remove and word]\n",
    "\n",
    "# Remove stop words\n",
    "tokens_cleaned = [t for t in tokens_without_removed_words if t not in _stop_words.ENGLISH_STOP_WORDS]\n",
    "\n",
    "# Get unique words and their count\n",
    "vocab = np.unique(tokens_cleaned)\n",
    "print(\"Tokenized content for Caucasian Donors:\")\n",
    "print(\"Unique words:\", vocab.shape)\n",
    "print(Counter(tokens_cleaned).most_common(50))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
